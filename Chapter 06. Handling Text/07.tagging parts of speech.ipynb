{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have text data and want to tag each word or character with its part of\n",
    "speech.\n",
    "\n",
    "Use NLTK’s pre-trained parts-of-speech tagger:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "# Use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "# Show parts of speech\n",
    "text_tagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of tuples with the word and the tag of the part of speech.\n",
    "NLTK uses the Penn Treebank parts for speech tags. Some examples of the Penn\n",
    "Treebank tags are\n",
    "\n",
    "![](./NLTKtag.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP','NNPS'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, tag) for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP','NNPS'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic situation would be that we have data where every observation\n",
    "contains a tweet and we want to convert those sentences into features for\n",
    "individual parts of speech h (e.g., a feature with 1 if a proper noun is present, and 0\n",
    "otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "\"Political science is an amazing field\",\n",
    "\"San Francisco is an awesome city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list\n",
    "tagged_tweets = []\n",
    "# Tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag = pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "# Use one-hot encoding to convert the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "one_hot_multi.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our text is English and not on a specialized topic (e.g., medicine) the simplest\n",
    "solution is to use NLTK’s pre-trained parts-of-speech tagger. However, if\n",
    "pos_tag is not very accurate, NLTK also gives us the ability to train our own\n",
    "tagger. The major downside of training a tagger is that we need a large corpus of\n",
    "text where the tag of each word is known. Constructing this tagged corpus is\n",
    "obviously labor intensive and is probably going to be a last resort.\n",
    "All that said, if we had a tagged corpus and wanted to train a tagger, the\n",
    "following is an example of how we could do it. The corpus we are using is the\n",
    "Brown Corpus, one of the most popular sources of tagged text. Here we use a\n",
    "backoff n-gram tagger, where n is the number of previous words we take into\n",
    "account when predicting a word’s part-of-speech tag. First we take into account\n",
    "the previous two words using TrigramTagger; if two words are not present, we\n",
    "“back off” and take into account the tag of the previous one word using\n",
    "BigramTagger, and finally if that fails we only look at the word itself using\n",
    "UnigramTagger. To examine the accuracy of our tagger, we split our text data\n",
    "into two parts, train our tagger on one part, and test how well it predicts the tags\n",
    "of the second part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanda\\AppData\\Local\\Temp\\ipykernel_28892\\2921306510.py:16: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram.evaluate(test)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
