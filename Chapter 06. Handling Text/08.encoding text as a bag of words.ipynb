{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have text data and want to create a set of features indicating the number of times an observation’s text contains a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use scikit-learn’s CountVectorizer:\n",
    "\n",
    "# Load library\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "'Sweden is best',\n",
    "'Germany beats both'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "# Show feature matrix\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is a sparse array, which is often necessary when we have a large\n",
    "amount of text. However, in our toy example we can use toarray to view a\n",
    "matrix of word counts for each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data in our solution was purposely small. In the real world, a single\n",
    "observation of text data could be the contents of an entire book! Since our bagof-words model creates a feature for every unique word in the data, the resulting\n",
    "matrix can contain thousands of features. This means that the size of the matrix\n",
    "can sometimes become very large in memory. However, luckily we can exploit a\n",
    "common characteristic of bag-of-words feature matrices to reduce the amount of\n",
    "data we need to store.\n",
    "Most words likely do not occur in most observations, and therefore bag-of-words\n",
    "feature matrices will contain mostly 0s as values. We call these types of matrices\n",
    "“sparse.” Instead of storing all values of the matrix, we can only store nonzero\n",
    "values and then assume all other values are 0. This will save us memory when\n",
    "we have large feature matrices. One of the nice features of CountVectorizer is\n",
    "that the output is a sparse matrix by default.\n",
    "CountVectorizer comes with a number of useful parameters to make creating\n",
    "bag-of-words feature matrices easy. First, while by default every feature is a\n",
    "word, that does not have to be the case. Instead we can set every feature to be the\n",
    "combination of two words (called a 2-gram) or even three words (3-gram).\n",
    "ngram_range sets the minimum and maximum size of our n-grams. For\n",
    "example, (2,3) will return all 2-grams and 3-grams. Second, we can easily\n",
    "remove low-information filler words using stop_words either with a built-in list\n",
    "or a custom list. Finally, we can restrict the words or phrases we want to consider\n",
    "to a certain list of words using vocabulary. For example, we could create a bagof-words feature matrix for only occurrences of country names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "stop_words=\"english\",\n",
    "vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "# View feature matrix\n",
    "bag.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the 1-grams and 2-grams\n",
    "count_2gram.vocabulary_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
