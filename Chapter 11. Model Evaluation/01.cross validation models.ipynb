{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to evaluate how well your model will work in the real world\n",
    "\n",
    "Create a pipeline that preprocesses the data, trains the model, and then evaluates\n",
    "it using cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits datasets\n",
    "digits=datasets.load_digits()\n",
    "# Create features matrix\n",
    "features=digits.data\n",
    "# Create target vector\n",
    "target=digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardizer\n",
    "standardizer=StandardScaler()\n",
    "\n",
    "# Create logistic regression object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "features, # Feature matrix\n",
    "target, # Target vector\n",
    "cv=kf, # Cross-validation technique\n",
    "scoring=\"accuracy\", # Loss function\n",
    "n_jobs=-1) # Use all CPU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693916821849783"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first consideration, evaluating supervised-learning models might appear\n",
    "straightforward: train a model and then calculate how well it did using some\n",
    "performance metric (accuracy, squared errors, etc.). However, this approach is\n",
    "fundamentally flawed. If we train a model using our data, and then evaluate how\n",
    "well it did on that data, we are not achieving our desired goal. Our goal is not to\n",
    "evaluate how well the model does on our training data, but how well it does on\n",
    "data it has never seen before (e.g., a new customer, a new crime, a new image).\n",
    "For this reason, our method of evaluation should help us understand how well\n",
    "models are able to make predictions from data they have never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy might be to hold off a slice of data for testing. This is called\n",
    "validation (or hold-out). In validation our observations (features and targets) are split into two sets, traditionally called the training set and the test set. We take\n",
    "the test set and put it off to the side, pretending that we have never seen it before.\n",
    "Next we train our model using our training set, using the features and target\n",
    "vector to teach the model how to make the best prediction. Finally, we simulate\n",
    "having never before seen external data by evaluating how our model trained on\n",
    "our training set performs on our test set. However, the validation approach has\n",
    "two major weaknesses. First, the performance of the model can be highly\n",
    "dependent on which few observations were selected for the test set. Second, the\n",
    "model is not being trained using all the available data, and not being evaluated\n",
    "on all the available data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better strategy, which overcomes these weaknesses, is called k-fold crossvalidation (KFCV). In KFCV, we split the data into k parts called “folds.” The\n",
    "model is then trained using k – 1 folds—combined into one training set—and\n",
    "then the last fold is used as a test set. We repeat this k times, each time using a\n",
    "different fold as the test set. The performance on the model for each of the k\n",
    "iterations is then averaged to produce an overall measurement.\n",
    "In our solution, we conducted k-fold cross-validation using 10 folds and\n",
    "outputted the evaluation scores to cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.98888889, 0.96111111, 0.94444444, 0.97777778,\n",
       "       0.98333333, 0.95555556, 0.98882682, 0.97765363, 0.93854749])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View score for all 10 folds\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three important points to consider when we are using KFCV. First,\n",
    "KFCV assumes that each observation was created independent from the other\n",
    "(i.e., the data is independent identically distributed [IID]). If the data is IID, it is\n",
    "a good idea to shuffle observations when assigning to folds. In scikit-learn we\n",
    "can set shuffle=True to perform shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, when we are using KFCV to evaluate a classifier, it is often beneficial to\n",
    "have folds containing roughly the same percentage of observations from each of\n",
    "the different target classes (called stratified k-fold). For example, if our target\n",
    "vector contained gender and 80% of the observations were male, then each fold\n",
    "would contain 80% male and 20% female observations. In scikit-learn, we can conduct stratified k-fold cross-validation by replacing the KFold class with\n",
    "StratifiedKFold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we are using validation sets or cross-validation, it is important to\n",
    "preprocess data based on the training set and then apply those transformations to\n",
    "both the training and test set. For example, when we fit our standardization\n",
    "object, standardizer, we calculate the mean and variance of only the training\n",
    "set. Then we apply that transformation (using transform) to both the training\n",
    "and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.1, random_state=1)\n",
    "# Fit standardizer to training set\n",
    "standardizer.fit(features_train)\n",
    "# Apply to both training and test sets\n",
    "features_train_std = standardizer.transform(features_train)\n",
    "features_test_std = standardizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn’s pipeline package makes this easy to do while using cross-validation\n",
    "techniques. We first create a pipeline that preprocesses the data (e.g.,\n",
    "standardizer) and then trains a model (logistic regression, logit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = make_pipeline(standardizer, logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run KFCV using that pipeline and scikit does all the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "features, # Feature matrix\n",
    "target, # Target vector\n",
    "cv=kf, # Cross-validation technique\n",
    "scoring=\"accuracy\", # Loss function\n",
    "n_jobs=-1) # Use all CPU scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_val_score comes with three parameters that we have not discussed that\n",
    "are worth noting. cv determines our cross-validation technique. K-fold is the\n",
    "most common by far, but there are others, like leave-one-out-cross-validation\n",
    "where the number of folds k equals the number of observations. The scoring\n",
    "parameter defines our metric for success, a number of which are discussed in\n",
    "other recipes in this chapter. Finally, n_jobs=-1 tells scikit-learn to use every\n",
    "core available. For example, if your computer has four cores (a common number\n",
    "for laptops), then scikit-learn will use all four cores at once to speed up the\n",
    "operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
