{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of features, you want to reduce the number of features while\n",
    "retaining the variance in the data\n",
    "\n",
    "Use principal component analysis with scikit’s PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 54\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "digits = datasets.load_digits()\n",
    "# Standardize the feature matrix\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "# Conduct PCA\n",
    "features_pca = pca.fit_transform(features)\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a popular linear dimensionality reduction\n",
    "technique. PCA projects observations onto the (hopefully fewer) principal\n",
    "components of the feature matrix that retain the most variance. PCA is an\n",
    "unsupervised technique, meaning that it does not use the information from the\n",
    "target vector and instead only considers the feature matrix.\n",
    "For a mathematical description of how PCA works, see the external resources\n",
    "listed at the end of this recipe. However, we can understand the intuition behind\n",
    "\n",
    "PCA using a simple example. In the following figure, our data contains two\n",
    "features, x1 and x2\n",
    ". Looking at the visualization, it should be clear that\n",
    "observations are spread out like a cigar, with a lot of length and very little height.\n",
    "More specifically, we can say that the variance of the “length” is significantly\n",
    "greater than the “height.” Instead of length and height, we refer to the\n",
    "“directions” with the most variance as the first principal component and the\n",
    "“direction” with the second-most variance as the second principal component\n",
    "(and so on).\n",
    "If we wanted to reduce our features, one strategy would be to project all\n",
    "observations in our 2D space onto the 1D principal component. We would lose\n",
    "the information captured in the second principal component, but in some\n",
    "situations that would be an acceptable trade-off. This is PCA.\n",
    "PCA is implemented in scikit-learn using the pca method. n_components has\n",
    "two operations, depending on the argument provided. If the argument is greater\n",
    "than 1, n_components will return that many features. This leads to the question\n",
    "of how to select the number of features that is optimal. Fortunately for us, if the\n",
    "argument to n_components is between 0 and 1, pca returns the minimum\n",
    "amount of features that retain that much variance. It is common to use values of\n",
    "0.95 and 0.99, meaning 95% and 99% of the variance of the original features has\n",
    "been retained, respectively. whiten=True transforms the values of each principal\n",
    "component so that they have zero mean and unit variance. Another parameter\n",
    "and argument is svd_solver=\"randomized\", which implements a stochastic\n",
    "algorithm to find the first principal components in often significantly less time.\n",
    "The output of our solution shows that PCA let us reduce our dimensionality by\n",
    "10 features while still retaining 99% of the information (variance) in the feature\n",
    "matrixs\n",
    "\n",
    "![](./pics/observations.jpg)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
