{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You suspect you have linearly inseparable data and want to reduce the\n",
    "dimensions.\n",
    "\n",
    "Use an extension of principal component analysis that uses kernels to allow for\n",
    "non-linear dimensionality reduction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 2\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "# Create linearly inseparable data\n",
    "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
    "# Apply kernal PCA with radius basis function (RBF) kernel\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is able to reduce the dimensionality of our feature matrix (e.g., the number\n",
    "of features). Standard PCA uses linear projection to reduce the features. If the\n",
    "data is linearly separable (i.e., you can draw a straight line or hyperplane\n",
    "between different classes) then PCA works well. However, if your data is not\n",
    "linearly separable (e.g., you can only separate classes using a curved decision\n",
    "boundary), the linear transformation will not work as well. In our solution we\n",
    "used scikit-learn’s make_circles to generate a simulated dataset with a target\n",
    "vector of two classes and two features. make_circles makes linearly\n",
    "inseparable data; specifically, one class is surrounded on all sides by the other\n",
    "class.\n",
    "\n",
    "![](./pics/linearyInseperable.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels allow us to project the linearly inseparable data into a higher dimension\n",
    "where it is linearly separable; this is called the kernel trick. Don’t worry if you\n",
    "don’t understand the details of the kernel trick; just think of kernels as different\n",
    "ways of projecting the data. There are a number of kernels we can use in scikit-learn’s kernelPCA, specified using the kernel parameter. A common kernel to\n",
    "use is the Gaussian radial basis function kernel rbf, but other options are the\n",
    "polynomial kernel (poly) and sigmoid kernel (sigmoid). We can even specify a\n",
    "linear projection (linear), which will produce the same results as standard\n",
    "PCA.\n",
    "One downside of kernel PCA is that there are a number of parameters we need to\n",
    "specify. For example, in Recipe 9.1 we set n_components to 0.99 to make PCA\n",
    "select the number of components to retain 99% of the variance. We don’t have\n",
    "this option in kernel PCA. Instead we have to define the number of parameters\n",
    "(e.g., n_components=1). Furthermore, kernels come with their own\n",
    "hyperparameters that we will have to set; for example, the radial basis function\n",
    "requires a gamma value.\n",
    "So how do we know which values to use? Through trial and error. Specifically\n",
    "we can train our machine learning model multiple times, each time with a\n",
    "different kernel or different value of the parameter. Once we find the\n",
    "combination of values that produces the highest quality predicted values, we are\n",
    "done. We will learn about this strategy in depth in Chapter 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
