{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to convert an image into an observation for machine learning\n",
    "\n",
    "Use NumPy’s flatten to convert the multidimensional array containing an\n",
    "image’s data into a vector containing the observation’s values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Load image as grayscale\n",
    "image = cv2.imread(\"../sim_data/images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resize image to 10 pixels by 10 pixels\n",
    "image_10x10 = cv2.resize(image, (10, 10))\n",
    "image_10x10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([133, 130, 130, 129, 130, 129, 129, 128, 128, 127, 135, 131, 131,\n",
       "       131, 130, 130, 129, 128, 128, 128, 134, 132, 131, 131, 130, 129,\n",
       "       129, 128, 130, 133, 132, 158, 130, 133, 130,  46,  97,  26, 132,\n",
       "       143, 141,  36,  54,  91,   9,   9,  49, 144, 179,  41, 142,  95,\n",
       "        32,  36,  29,  43, 113, 141, 179, 187, 141, 124,  26,  25, 132,\n",
       "       135, 151, 175, 174, 184, 143, 151,  38, 133, 134, 139, 174, 177,\n",
       "       169, 174, 155, 141, 135, 137, 137, 152, 169, 168, 168, 179, 152,\n",
       "       139, 136, 135, 137, 143, 159, 166, 171, 175], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert image data to one-dimensional vector. if we flatten the array, we get a vector of length 100 (10 multiplied by 10):\n",
    "image_10x10.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are presented as a grid of pixels. If an image is in grayscale, each pixel is\n",
    "presented by one value (i.e., pixel intensity: 1 if white, 0 if black). For example,\n",
    "imagine we have a 10 × 10–pixel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_10x10, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the feature data for our image that can be joined with the vectors from\n",
    "other images to create the data we will feed to our machine learning algorithms.\n",
    "If the image is in color, instead of each pixel being represented by one value, it is\n",
    "represented by multiple values (most often three) representing the channels (red,\n",
    "green, blue, etc.) that blend to make the final color of that pixel. For this reason,\n",
    "if our 10 × 10 image is in color, we will have 300 feature values for each\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image in color\n",
    "image_color = cv2.imread(\"../sim_data/images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
    "# Resize image to 10 pixels by 10 pixels\n",
    "image_color_10x10 = cv2.resize(image_color, (10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([197, 145,  85, 196, 143,  80, 196, 143,  80, 195, 142,  79, 197,\n",
       "       143,  80, 196, 142,  79, 196, 142,  79, 195, 141,  78, 195, 141,\n",
       "        78, 194, 140,  77, 199, 146,  89, 197, 144,  81, 197, 144,  81,\n",
       "       197, 144,  81, 197, 143,  80, 197, 143,  80, 196, 142,  79, 195,\n",
       "       141,  78, 195, 141,  78, 195, 141,  78, 198, 146,  86, 198, 144,\n",
       "        83, 197, 144,  81, 197, 144,  81, 196, 143,  80, 196, 143,  77,\n",
       "       196, 142,  79, 195, 141,  78, 195, 143,  82, 194, 147,  84, 198,\n",
       "       145,  82, 199, 166, 126, 197, 144,  77, 199, 146,  84, 186, 139,\n",
       "        89,  58,  52,  31, 109, 100,  88,  32,  28,  18, 174, 139, 103,\n",
       "       196, 152, 105, 201, 151,  97,  47,  41,  21,  68,  59,  39, 103,\n",
       "        94,  81,  24,  12,   0,  23,  12,   0,  87,  56,  22, 199, 154,\n",
       "       102, 183, 180, 173,  57,  46,  26, 200, 153, 100, 134, 106,  59,\n",
       "        40,  36,  23,  40,  41,  23,  34,  34,  18,  53,  47,  29, 124,\n",
       "       118,  99, 152, 145, 127, 190, 182, 169, 206, 190, 174, 200, 152,\n",
       "        96, 170, 134,  90,  33,  29,  17,  35,  28,  17, 198, 145,  82,\n",
       "       193, 147,  88, 199, 159, 117, 202, 178, 159, 202, 178, 156, 208,\n",
       "       187, 172, 201, 153, 100, 203, 160, 114,  43,  42,  27, 199, 145,\n",
       "        83, 198, 146,  85, 198, 150,  96, 206, 179, 153, 205, 181, 158,\n",
       "       204, 174, 146, 206, 179, 153, 206, 164, 119, 201, 152,  96, 198,\n",
       "       148,  87, 201, 149,  89, 199, 149,  89, 202, 160, 117, 204, 174,\n",
       "       145, 202, 174, 143, 203, 174, 143, 206, 182, 162, 203, 161, 116,\n",
       "       201, 151,  93, 200, 149,  87, 199, 148,  86, 199, 150,  88, 201,\n",
       "       154, 100, 203, 166, 128, 201, 173, 138, 201, 176, 150, 202, 179,\n",
       "       157], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_color_10x10.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "image_color_10x10.flatten().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major challenges of image processing and computer vision is that\n",
    "since every pixel location in a collection of images is a feature, as the images get\n",
    "larger, the number of features explodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image in grayscale\n",
    "image_256x256_gray = cv2.imread(\"../sim_data/images/plane_256x256.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "image_256x256_gray.flatten().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196608,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image in color\n",
    "image_256x256_color = cv2.imread(\"../sim_data/images/plane_256x256.jpg\", cv2.IMREAD_COLOR)\n",
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "image_256x256_color.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4f27d9b98637a93dfc92790c67a6714c8bfd98755bed637fb445c0198654de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
